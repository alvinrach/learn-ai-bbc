{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7cc0050-fa76-427a-a150-53f36c94de9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alvinrach/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5171fd26-f4d1-4e1a-bffa-caddc65246e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('https://raw.githubusercontent.com/alvinrach/learn-ai-bbc/main/BBC%20News%20Train.csv')\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/alvinrach/learn-ai-bbc/main/BBC%20News%20Test.csv')\n",
    "sample = pd.read_csv('https://raw.githubusercontent.com/alvinrach/learn-ai-bbc/main/BBC%20News%20Sample%20Solution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026f592a-1920-42bd-b0c8-d55cf4079753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1490 entries, 0 to 1489\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ArticleId  1490 non-null   int64 \n",
      " 1   Text       1490 non-null   object\n",
      " 2   Category   1490 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 35.1+ KB\n"
     ]
    }
   ],
   "source": [
    "d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce30b62-3ea6-4104-8fa5-517aedd87281",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.drop('ArticleId',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e030e92-bc03-43a6-a5f4-e3ca0672f482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>double eviction from big brother model caprice...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>dj double act revamp chart show dj duo jk and ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>weak dollar hits reuters revenues at media gro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>apple ipod family expands market apple has exp...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>santy worm makes unwelcome visit thousands of ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1490 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text       Category\n",
       "0     worldcom ex-boss launches defence lawyers defe...       business\n",
       "1     german business confidence slides german busin...       business\n",
       "2     bbc poll indicates economic gloom citizens in ...       business\n",
       "3     lifestyle  governs mobile choice  faster  bett...           tech\n",
       "4     enron bosses in $168m payout eighteen former e...       business\n",
       "...                                                 ...            ...\n",
       "1485  double eviction from big brother model caprice...  entertainment\n",
       "1486  dj double act revamp chart show dj duo jk and ...  entertainment\n",
       "1487  weak dollar hits reuters revenues at media gro...       business\n",
       "1488  apple ipod family expands market apple has exp...           tech\n",
       "1489  santy worm makes unwelcome visit thousands of ...           tech\n",
       "\n",
       "[1490 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d0fb13-40f4-4f73-add6-c8a5a02572ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txtprocess(txt):\n",
    "    txt = str(txt).lower()\n",
    "    txt = contractions.fix(txt)\n",
    "\n",
    "    txt = re.sub(r'[^a-zA-Z]', ' ', txt)\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "\n",
    "    txt = ' '.join(txt.split())\n",
    "\n",
    "    return txt\n",
    "\n",
    "d['Text'] = d['Text'].apply(txtprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fcf4183-9772-460a-a4f7-4abf641fb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# kayak you'll gitu masih ada ' nya , apa bagusnya sebelum txtprocess, tapi kecil semua sih\n",
    "def remove_stopwords(txt):\n",
    "    no_stopword_txt = [w for w in txt.split() if not w in stop_words]\n",
    "    return ' '.join(no_stopword_txt)\n",
    "\n",
    "d['Text'] = d['Text'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d88755f-2500-4038-bbad-3a7883ba9a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worldcom ex boss launches defence lawyers defe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bbc poll indicates economic gloom citizens maj...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lifestyle governs mobile choice faster better ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enron bosses payout eighteen former enron dire...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>double eviction big brother model caprice holb...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>dj double act revamp chart show dj duo jk joel...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>weak dollar hits reuters revenues media group ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>apple ipod family expands market apple expande...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>santy worm makes unwelcome visit thousands web...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1490 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  business  \\\n",
       "0     worldcom ex boss launches defence lawyers defe...         1   \n",
       "1     german business confidence slides german busin...         1   \n",
       "2     bbc poll indicates economic gloom citizens maj...         1   \n",
       "3     lifestyle governs mobile choice faster better ...         0   \n",
       "4     enron bosses payout eighteen former enron dire...         1   \n",
       "...                                                 ...       ...   \n",
       "1485  double eviction big brother model caprice holb...         0   \n",
       "1486  dj double act revamp chart show dj duo jk joel...         0   \n",
       "1487  weak dollar hits reuters revenues media group ...         1   \n",
       "1488  apple ipod family expands market apple expande...         0   \n",
       "1489  santy worm makes unwelcome visit thousands web...         0   \n",
       "\n",
       "      entertainment  politics  sport  tech  \n",
       "0                 0         0      0     0  \n",
       "1                 0         0      0     0  \n",
       "2                 0         0      0     0  \n",
       "3                 0         0      0     1  \n",
       "4                 0         0      0     0  \n",
       "...             ...       ...    ...   ...  \n",
       "1485              1         0      0     0  \n",
       "1486              1         0      0     0  \n",
       "1487              0         0      0     0  \n",
       "1488              0         0      0     1  \n",
       "1489              0         0      0     1  \n",
       "\n",
       "[1490 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = pd.get_dummies(d.Category, dtype=int)\n",
    "d_new = pd.concat([d, category], axis=1)\n",
    "d_new = d_new.drop('Category', axis=1)\n",
    "d_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d939af3-4adf-45ea-87ca-7457b92aa4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = d_new['Text'].values\n",
    "label = d_new[category.columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0164fa39-d44e-4ef6-ab41-81b041f913b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvinrach/.jupytervenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize directly (no fit_on_texts needed - it's already pretrained!)\n",
    "tokens = tokenizer(article.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Split the tokenized data\n",
    "padded_train, padded_test, y_train, y_test = train_test_split(\n",
    "    tokens['input_ids'], label, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f7d4df5-2d55-45ec-be9a-5357cf8fea2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101, 11865,  6562,  ...,     0,     0,     0],\n",
       "         [  101,  4121,  5481,  ...,     0,     0,     0],\n",
       "         [  101,  7206,  3404,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 23413,  2229,  ...,     0,     0,     0],\n",
       "         [  101,  3153,  2189,  ...,     0,     0,     0],\n",
       "         [  101,  3306,  3940,  ...,     0,     0,     0]]),\n",
       " array([[0, 0, 0, 1, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e808ccb6-2f71-46a8-960c-8861d45f16d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e212d7a-54a9-443d-9d83-ece8bad697c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# katanya sih biar mimic keras kita harus mimic weight nya keras dan pake forget gate bias nya keras\n",
    "# ini yang terbaik\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=500, hidden_dim=64, output_dim=5):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size+1, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        # compute lengths of non-padded tokens\n",
    "        lengths = torch.sum(x.abs().sum(dim=2) != 0, dim=1)  # or use original input: x_input != 0\n",
    "        # pack the sequence\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        # pass through LSTM\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed)\n",
    "        # unpack if needed (not necessary if just taking last hidden)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        # take last hidden state for classification\n",
    "        x = h_n[-1]  # [batch_size, hidden_dim]\n",
    "        x = self.fc(x)  # [batch_size, output_dim]\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = MyModel(vocab_size)\n",
    "\n",
    "# 4) Weight initialization to mimic Keras\n",
    "def init_lstm_like_keras(m):\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        # Keras embed init is usually uniform small; this is OK\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
    "        if m.padding_idx is not None:\n",
    "            with torch.no_grad():\n",
    "                m.weight[m.padding_idx].fill_(0)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    if isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)   # kernel ~ glorot\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)       # recurrent ~ orthogonal\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "\n",
    "model.apply(init_lstm_like_keras)\n",
    "\n",
    "# 5) Set forget-gate bias = 1 (handles bias_ih + bias_hh)\n",
    "for names in model.lstm._all_weights:\n",
    "    for name in names:\n",
    "        if 'bias' in name:\n",
    "            bias = getattr(model.lstm, name)\n",
    "            n = bias.size(0)\n",
    "            # gates are i, f, g, o => forget gate slice is n//4 to n//4*2\n",
    "            start = n // 4\n",
    "            end = start + n // 4\n",
    "            with torch.no_grad():\n",
    "                bias[start:end].fill_(1.0)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # same as categorical_crossentropy\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, eps=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "813504f8-7c17-47f4-9de2-7c833a6f1dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/60, Train Loss: 1.5462, Val Loss: 1.4764, Val Acc: 0.4799\n",
      "Epoch 2/60, Train Loss: 1.4291, Val Loss: 1.3843, Val Acc: 0.5738\n",
      "Epoch 3/60, Train Loss: 1.3124, Val Loss: 1.3364, Val Acc: 0.5503\n",
      "Epoch 4/60, Train Loss: 1.2475, Val Loss: 1.2360, Val Acc: 0.6409\n",
      "Epoch 5/60, Train Loss: 1.1760, Val Loss: 1.1996, Val Acc: 0.8255\n",
      "Epoch 6/60, Train Loss: 1.0698, Val Loss: 1.1633, Val Acc: 0.8054\n",
      "Epoch 7/60, Train Loss: 1.0025, Val Loss: 1.0639, Val Acc: 0.8658\n",
      "Epoch 8/60, Train Loss: 0.9476, Val Loss: 1.0032, Val Acc: 0.9295\n",
      "Epoch 9/60, Train Loss: 0.9247, Val Loss: 0.9998, Val Acc: 0.9161\n",
      "Epoch 10/60, Train Loss: 0.9182, Val Loss: 0.9861, Val Acc: 0.9228\n",
      "Epoch 11/60, Train Loss: 0.9129, Val Loss: 0.9831, Val Acc: 0.9262\n",
      "Epoch 12/60, Train Loss: 0.9161, Val Loss: 0.9907, Val Acc: 0.9228\n",
      "Epoch 13/60, Train Loss: 0.9164, Val Loss: 0.9870, Val Acc: 0.9262\n",
      "Epoch 14/60, Train Loss: 0.9133, Val Loss: 0.9788, Val Acc: 0.9329\n",
      "Epoch 15/60, Train Loss: 0.9102, Val Loss: 0.9760, Val Acc: 0.9362\n",
      "Epoch 16/60, Train Loss: 0.9082, Val Loss: 0.9749, Val Acc: 0.9329\n",
      "Epoch 17/60, Train Loss: 0.9081, Val Loss: 0.9741, Val Acc: 0.9329\n",
      "Epoch 18/60, Train Loss: 0.9079, Val Loss: 0.9734, Val Acc: 0.9329\n",
      "Epoch 19/60, Train Loss: 0.9078, Val Loss: 0.9728, Val Acc: 0.9329\n",
      "Epoch 20/60, Train Loss: 0.9077, Val Loss: 0.9723, Val Acc: 0.9329\n",
      "Epoch 21/60, Train Loss: 0.9076, Val Loss: 0.9718, Val Acc: 0.9329\n",
      "Epoch 22/60, Train Loss: 0.9075, Val Loss: 0.9714, Val Acc: 0.9329\n",
      "Epoch 23/60, Train Loss: 0.9075, Val Loss: 0.9710, Val Acc: 0.9329\n",
      "Epoch 24/60, Train Loss: 0.9074, Val Loss: 0.9706, Val Acc: 0.9362\n",
      "Epoch 25/60, Train Loss: 0.9074, Val Loss: 0.9702, Val Acc: 0.9362\n",
      "Epoch 26/60, Train Loss: 0.9073, Val Loss: 0.9698, Val Acc: 0.9396\n",
      "Epoch 27/60, Train Loss: 0.9073, Val Loss: 0.9694, Val Acc: 0.9396\n",
      "Epoch 28/60, Train Loss: 0.9072, Val Loss: 0.9663, Val Acc: 0.9430\n",
      "Epoch 29/60, Train Loss: 0.9072, Val Loss: 0.9660, Val Acc: 0.9463\n",
      "Epoch 30/60, Train Loss: 0.9072, Val Loss: 0.9658, Val Acc: 0.9463\n",
      "Epoch 31/60, Train Loss: 0.9071, Val Loss: 0.9656, Val Acc: 0.9463\n",
      "Epoch 32/60, Train Loss: 0.9071, Val Loss: 0.9655, Val Acc: 0.9463\n",
      "Epoch 33/60, Train Loss: 0.9071, Val Loss: 0.9653, Val Acc: 0.9463\n",
      "Epoch 34/60, Train Loss: 0.9070, Val Loss: 0.9651, Val Acc: 0.9463\n",
      "Epoch 35/60, Train Loss: 0.9070, Val Loss: 0.9650, Val Acc: 0.9463\n",
      "Epoch 36/60, Train Loss: 0.9070, Val Loss: 0.9648, Val Acc: 0.9463\n",
      "Epoch 37/60, Train Loss: 0.9070, Val Loss: 0.9647, Val Acc: 0.9463\n",
      "Epoch 38/60, Train Loss: 0.9070, Val Loss: 0.9646, Val Acc: 0.9463\n",
      "Epoch 39/60, Train Loss: 0.9069, Val Loss: 0.9645, Val Acc: 0.9463\n",
      "Epoch 40/60, Train Loss: 0.9069, Val Loss: 0.9644, Val Acc: 0.9463\n",
      "Epoch 41/60, Train Loss: 0.9068, Val Loss: 0.9641, Val Acc: 0.9463\n",
      "Epoch 42/60, Train Loss: 0.9061, Val Loss: 0.9639, Val Acc: 0.9463\n",
      "Epoch 43/60, Train Loss: 0.9060, Val Loss: 0.9639, Val Acc: 0.9463\n",
      "Epoch 44/60, Train Loss: 0.9060, Val Loss: 0.9638, Val Acc: 0.9463\n",
      "Epoch 45/60, Train Loss: 0.9060, Val Loss: 0.9638, Val Acc: 0.9463\n",
      "Epoch 46/60, Train Loss: 0.9060, Val Loss: 0.9638, Val Acc: 0.9463\n",
      "Epoch 47/60, Train Loss: 0.9060, Val Loss: 0.9638, Val Acc: 0.9430\n",
      "Epoch 48/60, Train Loss: 0.9060, Val Loss: 0.9637, Val Acc: 0.9430\n",
      "Epoch 49/60, Train Loss: 0.9060, Val Loss: 0.9637, Val Acc: 0.9430\n",
      "Epoch 50/60, Train Loss: 0.9060, Val Loss: 0.9637, Val Acc: 0.9430\n",
      "Epoch 51/60, Train Loss: 0.9060, Val Loss: 0.9637, Val Acc: 0.9430\n",
      "Epoch 52/60, Train Loss: 0.9060, Val Loss: 0.9637, Val Acc: 0.9430\n",
      "Epoch 53/60, Train Loss: 0.9059, Val Loss: 0.9637, Val Acc: 0.9430\n",
      "Epoch 54/60, Train Loss: 0.9059, Val Loss: 0.9637, Val Acc: 0.9430\n",
      "Epoch 55/60, Train Loss: 0.9059, Val Loss: 0.9638, Val Acc: 0.9430\n",
      "Epoch 56/60, Train Loss: 0.9059, Val Loss: 0.9638, Val Acc: 0.9430\n",
      "Epoch 57/60, Train Loss: 0.9059, Val Loss: 0.9638, Val Acc: 0.9430\n",
      "Epoch 58/60, Train Loss: 0.9059, Val Loss: 0.9639, Val Acc: 0.9430\n",
      "Epoch 59/60, Train Loss: 0.9059, Val Loss: 0.9639, Val Acc: 0.9430\n",
      "Epoch 60/60, Train Loss: 0.9059, Val Loss: 0.9640, Val Acc: 0.9430\n"
     ]
    }
   ],
   "source": [
    "#gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Convert your data to PyTorch tensors\n",
    "X_train_tensor = padded_train.clone().detach().long()\n",
    "y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.long)  # if one-hot\n",
    "X_val_tensor = padded_test.clone().detach().long()\n",
    "y_val_tensor = torch.tensor(np.argmax(y_test, axis=1), dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 15\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "num_epochs = 60\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "            correct += (y_pred.argmax(1) == y_batch).sum().item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = correct / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        best_model_state = model.state_dict()  # save best model\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            model.load_state_dict(best_model_state)  # restore best model\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "234bde76-6328-4817-b199-d32b7ffc88b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (embedding): Embedding(30523, 500, padding_idx=0)\n",
       "  (lstm): LSTM(500, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e09f1984-4326-4a39-b93d-f88882659192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1053, 18098,  ...,     0,     0,     0],\n",
       "        [  101,  4007,  3666,  ...,     0,     0,     0],\n",
       "        [  101,  8115,  2100,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  4368, 19244,  ...,     0,     0,     0],\n",
       "        [  101, 22989, 12390,  ...,     0,     0,     0],\n",
       "        [  101,  9068,  2724,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Text'] = test['Text'].apply(txtprocess)\n",
    "test['Text'] = test['Text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "testtext = test['Text'].values\n",
    "\n",
    "paddedtesttext = tokenizer(testtext.tolist(), padding=True, truncation=True, return_tensors='pt')['input_ids']\n",
    "paddedtesttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5187a39b-90b4-4533-aaf3-d2917870b940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tsunami hit sri lanka banks sri lanka banks face hard times following december tsunami disaster officials warned sri lanka banks association said waves killed people also washed away huge amounts property securing loans according estimate much loans made private banks clients disaster zone written damaged state owned lenders may even worse hit said association estimates private banking sector bn rupees loans outstanding disaster zone one hand banks dealing death customers along damaged destroyed collateral extending cheap loans rebuilding recovery well giving clients time repay existing borrowing combination means revenue shortfall slba chairman commercial bank managing director al gooneratne told news conference banks given moratoriums collecting interest least quarter said public sector one ten state owned people bank customers south sri lanka affected bank spokesman told reuters estimated bank loss bn rupees'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtext[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283d45c-e637-4bb0-b19b-cd09ed6dc33a",
   "metadata": {},
   "source": [
    "# 1 First method to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87e13285-e4cb-44de-9f56-e198ce422bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Save entire model (less common in production)\n",
    "torch.save(model, '1_complete_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b92875c-cfef-4223-bdc7-10ac2bda6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load, you MUST have:\n",
    "# - Exact same Python environment\n",
    "# - Same PyTorch version\n",
    "# - Same model class definition imported\n",
    "# - All dependencies available\n",
    "# - Python Dependency hell\n",
    "# - Deployment environment issues\n",
    "# - Security concerns\n",
    "# - Platform Limitation (cant in c++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be3257e8-5d15-4967-94bd-887e5327aedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "loaded_model = torch.load(\"1_complete_model.pth\", weights_only=False)\n",
    "loaded_model.eval()\n",
    "\n",
    "x_test = paddedtesttext[[9]]\n",
    "with torch.no_grad():\n",
    "    x_test = x_test.to(device)\n",
    "    logits = loaded_model(x_test)\n",
    "    pred = logits.argmax(dim=1).item()\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f1c68-1d6f-40d1-ac74-95f9e8a4b9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3ab410d-1448-48ce-b089-25de031142a6",
   "metadata": {},
   "source": [
    "# 2 Second Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c1cb8-7b38-4b18-bd83-14028208b76f",
   "metadata": {},
   "source": [
    "This method needs to rebuild class architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d39f2f4-ba9d-43e0-a238-17d97e17f0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Save state dict (recommended for training checkpoints)\n",
    "torch.save(model.state_dict(), '2_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45833e04-31a8-4c3b-a172-cc2bb9758a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it needs rebuilding architecture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=500, hidden_dim=64, output_dim=5):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size+1, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        # compute lengths of non-padded tokens\n",
    "        lengths = torch.sum(x.abs().sum(dim=2) != 0, dim=1)  # or use original input: x_input != 0\n",
    "        # pack the sequence\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        # pass through LSTM\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed)\n",
    "        # unpack if needed (not necessary if just taking last hidden)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        # take last hidden state for classification\n",
    "        x = h_n[-1]  # [batch_size, hidden_dim]\n",
    "        x = self.fc(x)  # [batch_size, output_dim]\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "vocab_size = tokenizer.vocab_size\n",
    "loaded_model = MyModel(vocab_size)\n",
    "\n",
    "# 4) Weight initialization to mimic Keras\n",
    "def init_lstm_like_keras(m):\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        # Keras embed init is usually uniform small; this is OK\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
    "        if m.padding_idx is not None:\n",
    "            with torch.no_grad():\n",
    "                m.weight[m.padding_idx].fill_(0)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    if isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)   # kernel ~ glorot\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)       # recurrent ~ orthogonal\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "\n",
    "loaded_model.apply(init_lstm_like_keras)\n",
    "\n",
    "# 5) Set forget-gate bias = 1 (handles bias_ih + bias_hh)\n",
    "for names in loaded_model.lstm._all_weights:\n",
    "    for name in names:\n",
    "        if 'bias' in name:\n",
    "            bias = getattr(loaded_model.lstm, name)\n",
    "            n = bias.size(0)\n",
    "            # gates are i, f, g, o => forget gate slice is n//4 to n//4*2\n",
    "            start = n // 4\n",
    "            end = start + n // 4\n",
    "            with torch.no_grad():\n",
    "                bias[start:end].fill_(1.0)\n",
    "\n",
    "#gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Move model to GPU\n",
    "loaded_model = loaded_model.to(device)\n",
    "\n",
    "loaded_model.load_state_dict(torch.load(\"2_model_weights.pth\"))\n",
    "loaded_model.eval()\n",
    "\n",
    "x_test = paddedtesttext[[9]]\n",
    "with torch.no_grad():\n",
    "    x_test = x_test.to(device)\n",
    "    logits = loaded_model(x_test)\n",
    "    pred = logits.argmax(dim=1).item()\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd41de-8425-4d48-9c60-15d15f2847c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa4637e-aedd-421e-9602-b4675815222e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e828538-b1de-470e-a37b-2b4a934be993",
   "metadata": {},
   "source": [
    "# 3 Third Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cebd86b-b600-44a9-a410-db3a29fa6f36",
   "metadata": {},
   "source": [
    "### 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19807c34-1641-4178-816e-0381770f63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TorchScript creates a serialized representation that includes:\n",
    "# - Model architecture\n",
    "# - Trained weights\n",
    "# - Computation graph\n",
    "# - NO Python dependencies!\n",
    "# - Self-contained\n",
    "# - Cross-Platform Deployment (include C++)\n",
    "# - Performance Optimized\n",
    "# - Version Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3badbebd-261e-4c99-8642-99748cd10774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# // Can even load in pure C++ applications!\n",
    "# #include <torch/script.h>\n",
    "\n",
    "# torch::jit::script::Module model = torch::jit::load(\"model.pt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bb3cb9c-d2e5-4f7c-87cb-1799ba4a4782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (embedding): Embedding(30523, 500, padding_idx=0)\n",
       "  (lstm): LSTM(500, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 3: Save for production with TorchScript (RECOMMENDED FOR PRODUCTION) \n",
    "model.eval() # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b756bd8-ce96-4088-b55f-e93660290639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracing works by running the model once with a sample input and recording all tensor operations that happen.\n",
    "# The traced graph is then saved as a TorchScript model.\n",
    "# That means you must provide a real example input tensor (with the same shape/type as your real data).\n",
    "# Without an input, TorchScript has no idea what the computation graph looks like.\n",
    "\n",
    "x_test = paddedtesttext[[9]].to(device)\n",
    "\n",
    "# Option 3a: Tracing (most common)\n",
    "traced_model = torch.jit.trace(model, x_test)\n",
    "traced_model.save('3a_model_traced.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f12da0d-fb53-41c2-8cfa-955c9e056011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "loaded_model = torch.jit.load(\"3a_model_traced.pt\")\n",
    "loaded_model.eval()\n",
    "\n",
    "x_test = paddedtesttext[[9]]\n",
    "with torch.no_grad():\n",
    "    x_test = x_test.to(device)\n",
    "    logits = loaded_model(x_test)\n",
    "    pred = logits.argmax(dim=1).item()\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993e5c1-afcc-4f21-af1c-cc54faef76dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a8cb346-e820-4672-a625-3704555c14e5",
   "metadata": {},
   "source": [
    "### 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdddb7ca-d41b-487f-9676-c2c99b72f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3b: Scripting (better for models with control flow)\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save('3b_model_scripted.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9534bd85-ad71-40a4-9bc3-150ed6817ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "loaded_model = torch.jit.load(\"3b_model_scripted.pt\")\n",
    "loaded_model.eval()\n",
    "\n",
    "x_test = paddedtesttext[[9]]\n",
    "with torch.no_grad():\n",
    "    x_test = x_test.to(device)\n",
    "    logits = loaded_model(x_test)\n",
    "    pred = logits.argmax(dim=1).item()\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4089b-41cf-45ff-8274-6a3cc002df50",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8f159-8fd8-46c5-8520-d19795e967db",
   "metadata": {},
   "source": [
    "| Method | Use Case | Production Ready |\n",
    "|--------|----------|------------------|\n",
    "| Complete Model (1)| Research/Development | ❌ Python dependencies |\n",
    "| State Dict (2)| Training Checkpoints | ❌ Needs class definition |\n",
    "| TorchScript (3)| Production Serving | ✅ Self-contained |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834b9413-2926-4205-9f89-353637faf97a",
   "metadata": {},
   "source": [
    "### 3a or 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e211c4bf-544d-4579-b7f2-9ec1c3be7dce",
   "metadata": {},
   "source": [
    "Use Tracing (3a) when:\n",
    "\n",
    "- Standard feedforward networks (like our BBC classifier)\n",
    "- CNNs with fixed architecture\n",
    "- Transformers with fixed sequence length\n",
    "- Models without dynamic branching\n",
    "- You want maximum performance optimization\n",
    "\n",
    "Use Scripting (3b) when:\n",
    "\n",
    "- Models with if/else statements based on input\n",
    "- Dynamic RNNs with variable sequence lengths\n",
    "- Models with loops that depend on data\n",
    "- Control flow that changes based on input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bde70ce2-df4a-48f5-9df2-8fb04b19b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3a\n",
    "# class BBCNewsClassifier(nn.Module):\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)        # Always executed\n",
    "#         x = self.relu(x)       # Always executed  \n",
    "#         x = self.dropout(x)    # Always executed\n",
    "#         x = self.fc2(x)        # Always executed\n",
    "#         return x               # Always executed\n",
    "\n",
    "# # Perfect for tracing - same path every time\n",
    "# traced = torch.jit.trace(model, example_input)  # ✅ Use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ba2d748-b235-482b-982f-c04a5ec07e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3b\n",
    "# class DynamicModel(nn.Module):\n",
    "#     def forward(self, x):\n",
    "#         if x.size(1) > 100:           # Dynamic branching!\n",
    "#             x = self.large_path(x)\n",
    "#         else:\n",
    "#             x = self.small_path(x)\n",
    "        \n",
    "#         for i in range(x.size(0)):    # Dynamic loops!\n",
    "#             x[i] = self.process_item(x[i])\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# # Tracing would only capture ONE path\n",
    "# # Scripting captures ALL possible paths\n",
    "# scripted = torch.jit.script(model)  # ✅ Use this for dynamic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48649c9a-7348-4d4b-b12f-14b1ddcf7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory is almost the same for each model but ofc those are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68c5142c-df61-434d-bb2c-f56d45eacdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 240828\n",
      "-rw-rw-r-- 1 alvinrach alvinrach 61631287 Aug 31 17:08 1_complete_model.pth\n",
      "-rw-rw-r-- 1 alvinrach alvinrach    25368 Aug 31 17:07 1_Deployment_4.ipynb\n",
      "-rw-rw-r-- 1 alvinrach alvinrach 61629677 Aug 31 17:08 2_model_weights.pth\n",
      "-rw-rw-r-- 1 alvinrach alvinrach 61638369 Aug 31 17:08 3a_model_traced.pt\n",
      "-rw-rw-r-- 1 alvinrach alvinrach 61655091 Aug 31 17:08 3b_model_scripted.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2ca17-789c-4e4d-9506-d07479521c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
